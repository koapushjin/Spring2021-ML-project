{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(true, pred):\n",
    "    error = np.square(np.subtract(true,pred)).mean(axis=0)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"ml-weibo-project-/raw_data.xlsx\",skiprows=1,usecols=[\"微博正文\",\"点赞数\",\"转发数\",\"评论数\"])\n",
    "df = df.rename(columns = {'微博正文':'text', '点赞数':'like', '转发数':'comment','评论数':'forward'}, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>like</th>\n",
       "      <th>comment</th>\n",
       "      <th>forward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【#你好，明天#】岁末年初，即便你的年终盘点一言难尽，但新年的flag仍然计日可期。需要抓住...</td>\n",
       "      <td>5389</td>\n",
       "      <td>1997</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>【今天，发条微博，向这些“闪亮的名字”致敬】南仁东、林俊德、张超、王继才、黄群、宋月才、姜开...</td>\n",
       "      <td>6826</td>\n",
       "      <td>1268</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>【夜读：这一年，谢谢自己】转眼间，2018年就要过去了。有时候我们很怕，怕自己的成长速度跟不...</td>\n",
       "      <td>6565</td>\n",
       "      <td>4326</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【小调查：跨年夜，你如何度过的？】#2018最后一天#，你是如何度过的？陪在家人身边？宅在家...</td>\n",
       "      <td>919</td>\n",
       "      <td>2562</td>\n",
       "      <td>5281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【揭秘！习主席书架上的新变化】今晚，习近平主席发表了#2019新年贺词#。这一次，习主席书架...</td>\n",
       "      <td>6513</td>\n",
       "      <td>1758</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  like  comment  forward\n",
       "0  【#你好，明天#】岁末年初，即便你的年终盘点一言难尽，但新年的flag仍然计日可期。需要抓住...  5389     1997      406\n",
       "1  【今天，发条微博，向这些“闪亮的名字”致敬】南仁东、林俊德、张超、王继才、黄群、宋月才、姜开...  6826     1268      289\n",
       "2  【夜读：这一年，谢谢自己】转眼间，2018年就要过去了。有时候我们很怕，怕自己的成长速度跟不...  6565     4326      580\n",
       "3  【小调查：跨年夜，你如何度过的？】#2018最后一天#，你是如何度过的？陪在家人身边？宅在家...   919     2562     5281\n",
       "4  【揭秘！习主席书架上的新变化】今晚，习近平主席发表了#2019新年贺词#。这一次，习主席书架...  6513     1758      440"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df['like'], bins=np.arange(0,50000,2000), color = 'brown')\n",
    "# plt.ylabel('number')\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(df['comment'], bins=np.arange(0,30000,2000), color = 'blue')\n",
    "# plt.ylabel('number')\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(df['forward'], bins=np.arange(0,10000,2000), color = 'red')\n",
    "# plt.ylabel('number')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-dd872dcbf353>:8: RuntimeWarning: divide by zero encountered in log\n",
      "  y = np.log(df[['like','forward','comment']])\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "f = open('ml-weibo-project-/tfidf3.txt', 'r')\n",
    "allsentences = f.readlines()\n",
    "\n",
    "for i in range(len(allsentences)):\n",
    "    allsentences[i] = allsentences[i].strip('\\n')\n",
    "# y = df[['like','comment','forward']]\n",
    "y = np.log(df[['like','forward','comment']])\n",
    "\n",
    "t = scipy.sparse.coo_matrix(y)\n",
    "for i, j, v in zip(t.row, t.col, t.data):\n",
    "    if (np.isnan(v) or np.isinf(v)):\n",
    "        y.iloc[i, j] = 0\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(allsentences, y, test_size=.3, train_size=.7, random_state=0)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 10000)\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(X_train))\n",
    "X_train_weight = tf_idf.toarray()\n",
    "\n",
    "tf_idf = tf_idf_transformer.transform(vectorizer.transform(X_test))\n",
    "X_test_weight = tf_idf.toarray() \n",
    "\n",
    "\n",
    "X_train_weight_nn = X_train_weight\n",
    "y_train_nn = y_train.to_numpy()\n",
    "\n",
    "X_test_weight_nn = X_test_weight\n",
    "y_test_nn = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77173, 10000)\n",
      "(77173, 3)\n"
     ]
    }
   ],
   "source": [
    "# print(X_train_weight_nn.shape)\n",
    "# print(y_train_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=10000, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable \n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt \n",
    "  \n",
    "class Net(torch.nn.Module): \n",
    "  def __init__(self, n_feature, n_hidden, n_output): \n",
    "    super(Net, self).__init__() \n",
    "    self.hidden = torch.nn.Linear(n_feature, n_hidden) \n",
    "    self.predict = torch.nn.Linear(n_hidden, n_output) \n",
    "  \n",
    "  def forward(self, x): \n",
    "    x = F.relu(self.hidden(x))\n",
    "    x = self.predict(x) \n",
    "    return x \n",
    "  \n",
    "net = Net(10000, 10, 3) \n",
    "print(net) \n",
    "net = net.double()\n",
    "  \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5) \n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "plt.ion()  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data (Dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "\n",
    "class MyDataSet():\n",
    "    def __init__(self, inputX, inputY):\n",
    "        super().__init__()\n",
    "        self.X = inputX\n",
    "        self.Y = inputY\n",
    "        self._len = len(self.X)\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.Y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 256\n",
    "epochNum = 20\n",
    "inputSlice = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_data:\n",
    "# X_train_weight_nn = X_train_weight_nn.astype(float)\n",
    "# y_train_nn = y_train_nn.astype(float)\n",
    "# train_x = torch.from_numpy(X_train_weight_nn[:inputSlice])\n",
    "# train_y = torch.from_numpy(y_train_nn[:inputSlice])\n",
    "\n",
    "# train_data_set = MyDataSet(train_x, train_y)\n",
    "# train_loader = DataLoader(dataset=train_data_set, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# # test_data:\n",
    "# X_test_weight_nn = X_test_weight_nn.astype(float)\n",
    "# y_test_nn = y_test_nn.astype(float)\n",
    "# test_x = torch.from_numpy(X_test_weight_nn[:inputSlice])\n",
    "# test_y = torch.from_numpy(y_test_nn[:inputSlice])\n",
    "\n",
    "# test_data_set = MyDataSet(test_x, test_y)\n",
    "# test_loader = DataLoader(dataset=test_data_set, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data:\n",
    "X_train_weight_nn = X_train_weight_nn.astype(float)\n",
    "y_train_nn = y_train_nn.astype(float)\n",
    "train_x = torch.from_numpy(X_train_weight_nn)\n",
    "train_y = torch.from_numpy(y_train_nn)\n",
    "\n",
    "train_data_set = MyDataSet(train_x, train_y)\n",
    "train_loader = DataLoader(dataset=train_data_set, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# test_data:\n",
    "X_test_weight_nn = X_test_weight_nn.astype(float)\n",
    "y_test_nn = y_test_nn.astype(float)\n",
    "test_x = torch.from_numpy(X_test_weight_nn)\n",
    "test_y = torch.from_numpy(y_test_nn)\n",
    "\n",
    "test_data_set = MyDataSet(test_x, test_y)\n",
    "test_loader = DataLoader(dataset=test_data_set, batch_size=batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMAE=[]\n",
    "testMAE=[]\n",
    "for epoch in range(epochNum):\n",
    "    # train\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # run models\n",
    "        pred_train_y = net(train_x)\n",
    "        loss = loss_function(pred_train_y, train_y) \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainMAE.append(loss)\n",
    "        \n",
    "    # test    \n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        # run models\n",
    "        pred_test_y = net(test_x)\n",
    "        loss = loss_function(pred_test_y, test_y) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        testMAE.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 256 epochNum: 20 inputSlice: noslice\n",
      "TRAIN ERROR!!\n",
      "[11670.063841632777, 5146.812287880117, 1211.3048044133116]\n",
      "TEST ERROR!!\n",
      "[11446.500611296187, 5098.407987872186, 1116.831272920347]\n"
     ]
    }
   ],
   "source": [
    "print('batchSize:',batchSize, 'epochNum:',epochNum, 'inputSlice:','noslice')\n",
    "pred_train_y_np = pred_train_y.detach().numpy()\n",
    "train_y_np = train_y.detach().numpy()\n",
    "\n",
    "train_error = np.absolute(np.subtract(np.exp(train_y_np),np.exp(pred_train_y_np))).mean(axis=0)\n",
    "#error[i][j]代表第i条text的赞(j=0)转(j=1)评(j=2)的误差\n",
    "print(\"TRAIN ERROR!!\")\n",
    "# print(train_error)\n",
    "print([train_error[0],train_error[2],train_error[1]])\n",
    "# print('like: ',train_error.sum(axis=0)[0])\n",
    "# print('forward: ',train_error.sum(axis=0)[1])\n",
    "# print('comment: ',train_error.sum(axis=0)[2])\n",
    "\n",
    "pred_test_y_np = pred_test_y.detach().numpy()\n",
    "test_y_np = test_y.detach().numpy()\n",
    "\n",
    "test_error = np.absolute(np.subtract(np.exp(test_y_np),np.exp(pred_test_y_np))).mean(axis=0)\n",
    "#error[i][j]代表第i条text的赞(j=0)转(j=1)评(j=2)的误差\n",
    "print(\"TEST ERROR!!\")\n",
    "# print(test_error)\n",
    "print([test_error[0],test_error[2],test_error[1]])\n",
    "\n",
    "# error.sum(axis=0)\n",
    "# print('like: ',test_error.sum(axis=0)[0])\n",
    "# print('forward: ',test_error.sum(axis=0)[1])\n",
    "# print('comment: ',test_error.sum(axis=0)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 256 epochNum: 1 inputSlice: 10000\n",
      "TRAIN ERROR!!\n",
      "[12590.17081083  1280.2614193   5473.38854839]\n",
      "TEST ERROR!!\n",
      "[12477.14964546  1347.34589175  9443.58714769]\n"
     ]
    }
   ],
   "source": [
    "# neuron: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 128 epochNum: 1 inputSlice: 20000\n",
      "TRAIN ERROR!!\n",
      "[12990.45075936  1345.25226159  5862.56904238]\n",
      "TEST ERROR!!\n",
      "like:  257353618.72064707\n",
      "forward:  26217073.951993212\n",
      "comment:  136723599.79981777\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2589, dtype=torch.float64, grad_fn=<L1LossBackward>) tensor(7.0656, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "[]\n",
      "[13134.86642161  1317.44547583  5836.74023574]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.5501395 , 6.17409849, 6.83437125],\n",
       "       [7.42997805, 6.07832344, 6.72076686],\n",
       "       [7.51336999, 6.14377439, 6.80254086],\n",
       "       ...,\n",
       "       [7.1439601 , 5.90179181, 6.52287035],\n",
       "       [7.25993323, 5.95999596, 6.59218973],\n",
       "       [7.14575197, 5.90507022, 6.53357377]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 128 epochNum: 1 inputSlice: 20000\n",
      "TRAIN ERROR!!\n",
      "[13134.86642161  1317.44547583  5836.74023574]\n",
      "TEST ERROR!!\n",
      "like:  252344190.9646321\n",
      "forward:  26419602.406009596\n",
      "comment:  136715710.59653315\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 128 epochNum: 1 inputSlice: 20000\n",
      "TRAIN ERROR!!\n",
      "[13145.15758016  1323.63056136  5847.76483202]\n",
      "TEST ERROR!!\n",
      "like:  252636293.21363625\n",
      "forward:  26601557.223065767\n",
      "comment:  136887992.18186474\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 256 epochNum: 1 inputSlice: 10000\n",
      "TRAIN ERROR!!\n",
      "like:  125277879.4885846\n",
      "forward:  12545197.052814486\n",
      "comment:  54410631.49697019\n",
      "TEST ERROR!!\n",
      "like:  122896861.8088555\n",
      "forward:  12881456.964935735\n",
      "comment:  93452552.88057627\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchSize: 128 epochNum: 1 inputSlice: 10000\n",
      "TRAIN ERROR!!\n",
      "like:  126571200.22719258\n",
      "forward:  13117773.349215588\n",
      "comment:  55238182.09323303\n",
      "TEST ERROR!!\n",
      "like:  121946858.26988766\n",
      "forward:  13547522.250401475\n",
      "comment:  94217450.57214229\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN ERROR!!\n",
      "like:  120357282.37726352\n",
      "forward:  12524944.590262374\n",
      "comment:  53188217.36421586\n",
      "TEST ERROR!!\n",
      "like:  120946834.02902414\n",
      "forward:  12795199.895387985\n",
      "comment:  92626643.96062796\n"
     ]
    }
   ],
   "source": [
    "# batchsize:128  epoch:1  input:all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch:1 input:1000 batchsize:256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
